# ASR Services Docker Compose
# Provider-based architecture: Run ONE provider at a time with configurable models
#
# Usage:
#   # Start NeMo provider (Parakeet, Canary)
#   ASR_MODEL=nvidia/parakeet-tdt-0.6b-v3 docker compose up nemo-asr -d
#
#   # Start Faster-Whisper provider
#   ASR_MODEL=Systran/faster-whisper-large-v3 docker compose up faster-whisper-asr -d
#
#   # Start Transformers provider (Hindi Whisper, HuggingFace models)
#   ASR_MODEL=openai/whisper-large-v3 docker compose up transformers-asr -d
#
#   # Start VibeVoice provider (speaker diarization)
#   docker compose up vibevoice-asr -d

services:
  # ============================================================================
  # NeMo Provider (Parakeet, Canary, etc.)
  # ============================================================================
  nemo-asr:
    build:
      context: .
      dockerfile: providers/nemo/Dockerfile
      args:
        PYTORCH_CUDA_VERSION: ${PYTORCH_CUDA_VERSION:-cu126}
    image: ${CHRONICLE_REGISTRY:-}chronicle-asr-nemo:${CHRONICLE_TAG:-latest}
    ports:
      - "${ASR_PORT:-8767}:8765"
    volumes:
      - ./model_cache:/models
      - ./debug:/app/debug
      - ./results:/app/results
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - HF_HOME=/models
      - ASR_MODEL=${ASR_MODEL:-nvidia/parakeet-tdt-0.6b-v3}
      # Legacy support
      - PARAKEET_MODEL=${PARAKEET_MODEL:-${ASR_MODEL:-nvidia/parakeet-tdt-0.6b-v3}}
      # Enhanced chunking configuration
      - CHUNKING_ENABLED=${CHUNKING_ENABLED:-true}
      - CHUNK_DURATION_SECONDS=${CHUNK_DURATION_SECONDS:-30.0}
      - OVERLAP_DURATION_SECONDS=${OVERLAP_DURATION_SECONDS:-5.0}
      - MIN_AUDIO_FOR_CHUNKING=${MIN_AUDIO_FOR_CHUNKING:-60.0}
      - CONFIDENCE_THRESHOLD=${CONFIDENCE_THRESHOLD:-0.8}
    restart: unless-stopped

  # ============================================================================
  # Faster-Whisper Provider (4-6x faster Whisper inference)
  # ============================================================================
  faster-whisper-asr:
    build:
      context: .
      dockerfile: providers/faster_whisper/Dockerfile
    image: ${CHRONICLE_REGISTRY:-}chronicle-asr-faster-whisper:${CHRONICLE_TAG:-latest}
    ports:
      - "${ASR_PORT:-8767}:8765"
    volumes:
      - ./model_cache:/models
      - ./debug:/app/debug
      - ./results:/app/results
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - HF_HOME=/models
      - ASR_MODEL=${ASR_MODEL:-Systran/faster-whisper-large-v3}
      - COMPUTE_TYPE=${COMPUTE_TYPE:-float16}
      - DEVICE=${DEVICE:-cuda}
      - DEVICE_INDEX=${DEVICE_INDEX:-0}
      - VAD_FILTER=${VAD_FILTER:-true}
      - LANGUAGE=${LANGUAGE:-}
    restart: unless-stopped

  # ============================================================================
  # VibeVoice Provider (Microsoft VibeVoice-ASR with speaker diarization)
  # Transformers-based approach with speaker diarization support
  # ============================================================================
  vibevoice-asr:
    build:
      context: .
      dockerfile: providers/vibevoice/Dockerfile
      args:
        PYTORCH_CUDA_VERSION: ${PYTORCH_CUDA_VERSION:-cu126}
    image: ${CHRONICLE_REGISTRY:-}chronicle-asr-vibevoice:${CHRONICLE_TAG:-latest}
    ports:
      - "${ASR_PORT:-8767}:8765"
    volumes:
      - ./model_cache:/models
      - ./debug:/app/debug
      - ./results:/app/results
      - ../../config:/app/config:ro
      - ./lora_adapters:/models/lora_adapters
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - HF_HOME=/models
      - ASR_MODEL=${ASR_MODEL:-microsoft/VibeVoice-ASR}
      - VIBEVOICE_LLM_MODEL=${VIBEVOICE_LLM_MODEL:-Qwen/Qwen2.5-7B}
      - VIBEVOICE_ATTN_IMPL=${VIBEVOICE_ATTN_IMPL:-sdpa}
      - DEVICE=${DEVICE:-cuda}
      - TORCH_DTYPE=${TORCH_DTYPE:-bfloat16}
      - MAX_NEW_TOKENS=${MAX_NEW_TOKENS:-8192}
      # Quantization: "4bit", "8bit", or "" (none). 4bit recommended for <=24GB VRAM.
      - QUANTIZATION=${QUANTIZATION:-4bit}
      # LoRA adapter: path to pre-trained adapter to auto-load on startup (optional)
      - LORA_ADAPTER_PATH=${LORA_ADAPTER_PATH:-}
      # Batching config: managed via config/defaults.yml (asr_services.vibevoice)
    dns:
      - 8.8.8.8
      - 8.8.4.4
    restart: unless-stopped

  # ============================================================================
  # Transformers Provider (Hindi Whisper, HuggingFace models)
  # ============================================================================
  transformers-asr:
    build:
      context: .
      dockerfile: providers/transformers/Dockerfile
    image: ${CHRONICLE_REGISTRY:-}chronicle-asr-transformers:${CHRONICLE_TAG:-latest}
    ports:
      - "${ASR_PORT:-8767}:8765"
    volumes:
      - ./model_cache:/models
      - ./debug:/app/debug
      - ./results:/app/results
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - HF_HOME=/models
      - ASR_MODEL=${ASR_MODEL:-openai/whisper-large-v3}
      - USE_FLASH_ATTENTION=${USE_FLASH_ATTENTION:-false}
      - DEVICE=${DEVICE:-cuda}
      - TORCH_DTYPE=${TORCH_DTYPE:-float16}
      - LANGUAGE=${LANGUAGE:-}
    restart: unless-stopped

  # ============================================================================
  # Qwen3-ASR vLLM Server (GPU model serving)
  # ============================================================================
  qwen3-asr:
    build:
      context: .
      dockerfile: providers/qwen3_asr/Dockerfile.vllm
    image: chronicle-asr-qwen3-vllm:latest
    ports:
      - "${ASR_VLLM_PORT:-8768}:8000"
    volumes:
      - ./model_cache:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - HF_HOME=/models
    command: >
      ${ASR_MODEL:-Qwen/Qwen3-ASR-1.7B}
      --gpu-memory-utilization ${QWEN3_GPU_MEM:-0.8}
      --host 0.0.0.0 --port 8000
      --max-model-len ${QWEN3_MAX_MODEL_LEN:-8192}
    restart: unless-stopped

  # ============================================================================
  # Qwen3-ASR Batch Wrapper (exposes /transcribe on standard ASR port)
  # Uses Dockerfile.full with ForcedAligner for word-level timestamps.
  # Without ForcedAligner, the backend has no words to create segments from.
  # ============================================================================
  qwen3-asr-wrapper:
    build:
      context: .
      dockerfile: providers/qwen3_asr/Dockerfile.full
    image: ${CHRONICLE_REGISTRY:-}chronicle-asr-qwen3-wrapper:${CHRONICLE_TAG:-latest}
    ports:
      - "${ASR_PORT:-8767}:8765"
    volumes:
      - ./model_cache:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - QWEN3_VLLM_URL=http://qwen3-asr:8000
      - ASR_MODEL=${ASR_MODEL:-Qwen/Qwen3-ASR-1.7B}
      - FORCED_ALIGNER_MODEL=${FORCED_ALIGNER_MODEL:-Qwen/Qwen3-ForcedAligner-0.6B}
    depends_on:
      - qwen3-asr
    restart: unless-stopped

  # ============================================================================
  # Qwen3-ASR Streaming Bridge (WebSocket â†’ vLLM SSE)
  # ============================================================================
  qwen3-asr-bridge:
    build:
      context: .
      dockerfile: providers/qwen3_asr/Dockerfile
    image: ${CHRONICLE_REGISTRY:-}chronicle-asr-qwen3-bridge:${CHRONICLE_TAG:-latest}
    ports:
      - "${ASR_STREAM_PORT:-8769}:8766"
    environment:
      - QWEN3_VLLM_URL=http://qwen3-asr:8000
      - ASR_MODEL=${ASR_MODEL:-Qwen/Qwen3-ASR-1.7B}
      - STREAM_INTERVAL_SECONDS=${STREAM_INTERVAL_SECONDS:-3}
    command: ["python", "-m", "providers.qwen3_asr.streaming_bridge", "--port", "8766"]
    depends_on:
      - qwen3-asr
    restart: unless-stopped

  # ============================================================================
  # Legacy Parakeet Service (backward compatibility)
  # ============================================================================
  parakeet-asr:
    build:
      context: .
      dockerfile: Dockerfile_Parakeet
      args:
        PYTORCH_CUDA_VERSION: ${PYTORCH_CUDA_VERSION:-cu126}
    image: parakeet-asr:latest
    ports:
      - "${PARAKEET_HOST_PORT:-8767}:${PARAKEET_CONTAINER_PORT:-8765}"
    volumes:
      - ./model_cache:/models
      - ./debug:/app/debug
      - ./results:/app/results
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - HF_HOME=/models
      - PARAKEET_MODEL=${PARAKEET_MODEL:-nvidia/parakeet-tdt-0.6b-v3}
      # Enhanced chunking configuration
      - CHUNKING_ENABLED=${CHUNKING_ENABLED:-true}
      - CHUNK_DURATION_SECONDS=${CHUNK_DURATION_SECONDS:-30.0}
      - OVERLAP_DURATION_SECONDS=${OVERLAP_DURATION_SECONDS:-5.0}
      - MIN_AUDIO_FOR_CHUNKING=${MIN_AUDIO_FOR_CHUNKING:-60.0}
      - CONFIDENCE_THRESHOLD=${CONFIDENCE_THRESHOLD:-0.8}
    restart: unless-stopped
