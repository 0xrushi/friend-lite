services:
  # Base Speaker Recognition Service Configuration
  speaker-service:
    &base-speaker-service
    profiles: ["cpu"]
    platform: linux/amd64
    build:
      context: .
      dockerfile: Dockerfile
      args:
        PYTORCH_CUDA_VERSION: ${PYTORCH_CUDA_VERSION:-cpu}
    image: speaker-recognition:latest
    env_file:
      - .env
    ports:
      - "${SPEAKER_SERVICE_PORT:-8085}:${SPEAKER_SERVICE_PORT:-8085}"
    volumes:
      - ./src:/app/src
      - ./model_cache:/models
      - ./audio_chunks:/app/audio_chunks
      - ./debug:/app/debug
      - ./speaker_data:/app/data
      - ../../config:/app/config:ro
    environment:
      - HF_HOME=/models
      - HF_TOKEN=${HF_TOKEN}
      - SIMILARITY_THRESHOLD=${SIMILARITY_THRESHOLD:-0.15}
      - SPEAKER_SERVICE_HOST=${SPEAKER_SERVICE_HOST:-0.0.0.0}
      - SPEAKER_SERVICE_PORT=${SPEAKER_SERVICE_PORT:-8085}
      - DEEPGRAM_API_KEY=${DEEPGRAM_API_KEY}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8085/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
          - speaker-service

  # GPU Profile Configuration
  speaker-service-gpu:
    <<: *base-speaker-service
    profiles: ["gpu"]
    build:
      context: .
      dockerfile: Dockerfile
      args:
        PYTORCH_CUDA_VERSION: ${PYTORCH_CUDA_VERSION:-cu126}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      default:
        aliases:
          - speaker-service

  # AMD Strix Halo (gfx1151 / Ryzen AI Max) Profile
  speaker-service-strixhalo:
    <<: *base-speaker-service
    profiles: ["strixhalo"]
    build:
      context: .
      dockerfile: Dockerfile.strixhalo
    image: speaker-recognition-strixhalo:latest
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    group_add:
      # Use numeric GIDs - group names (video/render) don't exist in vllm-therock image.
      # Find your system's GIDs with: getent group video render
      - "${VIDEO_GID:-985}"   # video group (Arch default; Ubuntu typically 44)
      - "${RENDER_GID:-989}"  # render group (Arch default; Ubuntu typically 107)
    environment:
      - HF_HOME=/models
      - HF_TOKEN=${HF_TOKEN}
      - SIMILARITY_THRESHOLD=${SIMILARITY_THRESHOLD:-0.15}
      - SPEAKER_SERVICE_HOST=${SPEAKER_SERVICE_HOST:-0.0.0.0}
      - SPEAKER_SERVICE_PORT=${SPEAKER_SERVICE_PORT:-8085}
      - DEEPGRAM_API_KEY=${DEEPGRAM_API_KEY}
      - HSA_OVERRIDE_GFX_VERSION=11.5.1
    networks:
      default:
        aliases:
          - speaker-service

  # React Web UI
  web-ui:
    platform: linux/amd64
    build:
      context: webui
      dockerfile: Dockerfile
    profiles: ["cpu", "gpu", "strixhalo"]
    ports:
      - "${REACT_UI_PORT:-5173}:${REACT_UI_PORT:-5173}"
    volumes:
      - ./webui/src:/app/src
      - ./webui/public:/app/public
      - ./webui/package.json:/app/package.json
      - ./webui/vite.config.ts:/app/vite.config.ts
      - ./webui/tailwind.config.js:/app/tailwind.config.js
      - ./webui/postcss.config.js:/app/postcss.config.js
      - ./webui/tsconfig.json:/app/tsconfig.json
    environment:
      - REACT_UI_HOST=${REACT_UI_HOST}
      - REACT_UI_PORT=${REACT_UI_PORT}
      - REACT_UI_HTTPS=${REACT_UI_HTTPS}
      - SPEAKER_SERVICE_HOST=${SPEAKER_SERVICE_HOST}
      - SPEAKER_SERVICE_PORT=${SPEAKER_SERVICE_PORT}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/app/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Caddy reverse proxy for unified HTTPS endpoint
  caddy:
    image: caddy:2-alpine
    profiles: ["cpu", "gpu", "strixhalo"]
    ports:
      - "8444:443"
      - "8081:80"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - ../../certs:/certs:ro
      - caddy_speaker_data:/data
      - caddy_speaker_config:/config
    depends_on:
      web-ui:
        condition: service_healthy
    restart: unless-stopped

# Shared network for cross-project communication
networks:
  default:
    name: chronicle-network
    external: true

volumes:
  caddy_speaker_data:
    driver: local
  caddy_speaker_config:
    driver: local
