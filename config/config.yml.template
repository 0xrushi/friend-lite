defaults:
  llm: openai-llm
  embedding: openai-embed
  stt: stt-deepgram
  # Transcription provider selection:
  # - stt-deepgram: Cloud-based (requires DEEPGRAM_API_KEY in .env)
  # - stt-parakeet-batch: Local ASR (requires Parakeet service running)
  tts: tts-http
  vector_store: vs-qdrant
models:
- name: emberfang-llm
  description: Emberfang One LLM
  model_type: llm
  model_provider: openai
  model_name: gpt-oss-20b-f16
  model_url: http://192.168.1.166:8084/v1
  api_key: '1234'
  model_params:
    temperature: 0.2
    max_tokens: 2000
  model_output: json
- name: emberfang-embed
  description: Emberfang embeddings (nomic-embed-text)
  model_type: embedding
  model_provider: openai
  model_name: nomic-embed-text-v1.5
  model_url: http://192.168.1.166:8084/v1
  api_key: '1234'
  embedding_dimensions: 768
  model_output: vector
- name: local-llm
  description: Local Ollama LLM
  model_type: llm
  model_provider: ollama
  api_family: openai
  model_name: llama3.1:latest
  model_url: http://localhost:11434/v1
  api_key: ${oc.env:OPENAI_API_KEY,ollama}
  model_params:
    temperature: 0.2
    max_tokens: 2000
  model_output: json
- name: local-embed
  description: Local embeddings via Ollama nomic-embed-text
  model_type: embedding
  model_provider: ollama
  api_family: openai
  model_name: nomic-embed-text:latest
  model_url: http://localhost:11434/v1
  api_key: ${oc.env:OPENAI_API_KEY,ollama}
  embedding_dimensions: 768
  model_output: vector
- name: openai-llm
  description: OpenAI GPT-4o-mini
  model_type: llm
  model_provider: openai
  api_family: openai
  model_name: gpt-4o-mini
  model_url: https://api.openai.com/v1
  api_key: ${oc.env:OPENAI_API_KEY,''}
  model_params:
    temperature: 0.2
    max_tokens: 2000
  model_output: json
- name: openai-embed
  description: OpenAI text-embedding-3-small
  model_type: embedding
  model_provider: openai
  api_family: openai
  model_name: text-embedding-3-small
  model_url: https://api.openai.com/v1
  api_key: ${oc.env:OPENAI_API_KEY,''}
  embedding_dimensions: 1536
  model_output: vector
- name: groq-llm
  description: Groq LLM via OpenAI-compatible API
  model_type: llm
  model_provider: groq
  api_family: openai
  model_name: llama-3.1-70b-versatile
  model_url: https://api.groq.com/openai/v1
  api_key: ${oc.env:GROQ_API_KEY,''}
  model_params:
    temperature: 0.2
    max_tokens: 2000
  model_output: json
- name: vs-qdrant
  description: Qdrant vector database
  model_type: vector_store
  model_provider: qdrant
  api_family: qdrant
  model_url: http://${oc.env:QDRANT_BASE_URL,qdrant}:${oc.env:QDRANT_PORT,6333}
  model_params:
    host: ${oc.env:QDRANT_BASE_URL,qdrant}
    port: ${oc.env:QDRANT_PORT,6333}
    collection_name: omi_memories
- name: stt-parakeet-batch
  description: Parakeet NeMo ASR (batch)
  model_type: stt
  model_provider: parakeet
  api_family: http
  model_url: http://${oc.env:PARAKEET_ASR_URL,172.17.0.1:8767}
  api_key: ''
  # Capabilities: what this provider can produce
  # - word_timestamps: Word-level timing data for alignment
  # - segments: Speaker segments (generic labels like Speaker 0)
  capabilities:
    - word_timestamps
    - segments
  operations:
    stt_transcribe:
      method: POST
      path: /transcribe
      content_type: multipart/form-data
      response:
        type: json
        extract:
          text: text
          words: words
          segments: segments
- name: stt-deepgram
  description: Deepgram Nova 3 (batch)
  model_type: stt
  model_provider: deepgram
  api_family: http
  model_url: https://api.deepgram.com/v1
  api_key: ${oc.env:DEEPGRAM_API_KEY,''}
  # Capabilities: what this provider can produce
  # - word_timestamps: Word-level timing data
  # - segments: Speaker segments with paragraphs
  # - diarization: Native speaker diarization (Speaker 0, Speaker 1, etc.)
  capabilities:
    - word_timestamps
    - segments
    - diarization
  operations:
    stt_transcribe:
      method: POST
      path: /listen
      headers:
        Authorization: Token ${oc.env:DEEPGRAM_API_KEY,''}
        Content-Type: audio/raw
      query:
        model: nova-3
        language: multi
        smart_format: 'true'
        punctuate: 'true'
        diarize: 'true'
        encoding: linear16
        sample_rate: 16000
        channels: '1'
      response:
        type: json
        extract:
          text: results.channels[0].alternatives[0].transcript
          words: results.channels[0].alternatives[0].words
          segments: results.channels[0].alternatives[0].paragraphs.paragraphs
- name: stt-vibevoice
  description: Microsoft VibeVoice ASR with speaker diarization
  model_type: stt
  model_provider: vibevoice
  api_family: http
  model_url: http://${oc.env:VIBEVOICE_ASR_URL,host.docker.internal:8767}
  api_key: ''
  # Capabilities: what this provider can produce
  # - segments: Speaker segments with diarization labels
  # - diarization: Built-in speaker diarization (no word timestamps)
  # Note: VibeVoice does NOT provide word_timestamps
  capabilities:
    - segments
    - diarization
  operations:
    stt_transcribe:
      method: POST
      path: /transcribe
      content_type: multipart/form-data
      timeout: 300
      # Per-read timeout for NDJSON batch progress responses.
      # Each batch window can take minutes; this timeout covers
      # the gap between successive progress lines.
      read_timeout: 600
      response:
        type: json
        extract:
          text: text
          segments: segments
- name: stt-faster-whisper
  description: Faster Whisper ASR (CTranslate2)
  model_type: stt
  model_provider: faster-whisper
  api_family: http
  model_url: http://${oc.env:FASTER_WHISPER_ASR_URL,host.docker.internal:8767}
  api_key: ''
  # Capabilities: what this provider can produce
  # - word_timestamps: Word-level timing data
  # - segments: Speaker segments
  capabilities:
    - word_timestamps
    - segments
  operations:
    stt_transcribe:
      method: POST
      path: /transcribe
      content_type: multipart/form-data
      response:
        type: json
        extract:
          text: text
          words: words
          segments: segments
- name: stt-transformers
  description: HuggingFace Transformers ASR
  model_type: stt
  model_provider: transformers
  api_family: http
  model_url: http://${oc.env:TRANSFORMERS_ASR_URL,host.docker.internal:8767}
  api_key: ''
  # Capabilities: what this provider can produce
  # - word_timestamps: Word-level timing data
  # - segments: Speaker segments
  capabilities:
    - word_timestamps
    - segments
  operations:
    stt_transcribe:
      method: POST
      path: /transcribe
      content_type: multipart/form-data
      response:
        type: json
        extract:
          text: text
          words: words
          segments: segments
- name: stt-nemo
  description: NVIDIA NeMo ASR (Parakeet, Canary)
  model_type: stt
  model_provider: nemo
  api_family: http
  model_url: http://${oc.env:NEMO_ASR_URL,host.docker.internal:8767}
  api_key: ''
  # Capabilities: what this provider can produce
  # - word_timestamps: Word-level timing data
  # - segments: Speaker segments
  capabilities:
    - word_timestamps
    - segments
  operations:
    stt_transcribe:
      method: POST
      path: /transcribe
      content_type: multipart/form-data
      response:
        type: json
        extract:
          text: text
          words: words
          segments: segments
- name: stt-qwen3-asr
  description: Qwen3-ASR via vLLM (batch, 52 languages)
  model_type: stt
  model_provider: qwen3-asr
  api_family: http
  model_url: http://${oc.env:QWEN3_ASR_URL,host.docker.internal:8767}
  api_key: ''
  capabilities:
    - multilingual
    - language_detection
  operations:
    stt_transcribe:
      method: POST
      path: /transcribe
      content_type: multipart/form-data
      response:
        type: json
        extract:
          text: text
          words: words
          segments: segments
- name: stt-smallest
  description: Smallest.ai Pulse STT (batch)
  model_type: stt
  model_provider: smallest
  api_family: http
  model_url: https://waves-api.smallest.ai/api/v1/pulse
  api_key: ${oc.env:SMALLEST_API_KEY,''}
  capabilities:
    - word_timestamps
    - diarization
  operations:
    stt_transcribe:
      method: POST
      path: /get_text
      headers:
        Authorization: Bearer ${oc.env:SMALLEST_API_KEY,''}
      query:
        model: pulse
        language: en
        word_timestamps: 'true'
      response:
        type: json
        extract:
          text: transcription
          words: words
          segments: utterances
- name: tts-http
  description: Generic JSON TTS endpoint
  model_type: tts
  model_provider: custom
  api_family: http
  model_url: http://localhost:9000
  operations:
    tts_synthesize:
      method: POST
      path: /synthesize
      headers:
        Content-Type: application/json
      response:
        type: json
- name: stt-deepgram-stream
  description: Deepgram Nova 3 streaming transcription over WebSocket
  model_type: stt_stream
  model_provider: deepgram
  api_family: websocket
  model_url: wss://api.deepgram.com/v1/listen
  api_key: ${oc.env:DEEPGRAM_API_KEY,''}
  capabilities:
    - diarization
  operations:
    query:
      model: nova-3
      language: multi
      smart_format: 'true'
      punctuate: 'true'
      diarize: 'true'
      encoding: linear16
      sample_rate: 16000
      channels: '1'
    end:
      message:
        type: CloseStream
    expect:
      interim_type: Results
      final_type: Results
      extract:
        text: channel.alternatives[0].transcript
        words: channel.alternatives[0].words
        segments: channel.alternatives[0].paragraphs.paragraphs
- name: stt-qwen3-asr-stream
  description: Qwen3-ASR streaming transcription via vLLM bridge
  model_type: stt_stream
  model_provider: qwen3-asr
  api_family: websocket
  model_url: ws://${oc.env:QWEN3_ASR_STREAM_URL,host.docker.internal:8769}
  api_key: ''
  operations:
    query: {}
    end:
      message:
        type: CloseStream
    expect:
      interim_type: interim
      final_type: final
      extract:
        text: text
- name: stt-smallest-stream
  description: Smallest.ai Pulse streaming transcription over WebSocket
  model_type: stt_stream
  model_provider: smallest
  api_family: websocket
  model_url: wss://waves-api.smallest.ai/api/v1/pulse/get_text
  api_key: ${oc.env:SMALLEST_API_KEY,''}
  capabilities:
    - word_timestamps
    - diarization
  operations:
    auth_prefix: Bearer
    query:
      language: en
      encoding: linear16
      sample_rate: 16000
      word_timestamps: 'true'
      diarize: 'true'
    end:
      message:
        type: finalize
    expect:
      interim_type: ''
      final_type: ''
      extract:
        text: transcript
        words: words
        segments: utterances
memory:
  provider: chronicle
  timeout_seconds: 1200
  extraction:
    enabled: true
    prompt: 'Extract important information from this conversation and return a JSON
      object with an array named "facts". Include personal preferences, plans, names,
      dates, locations, numbers, and key details. Keep items concise and useful.

      '
  openmemory_mcp:
    server_url: http://localhost:8765
    client_name: chronicle
    user_id: default
    timeout: 30
  obsidian:
    enabled: false
    neo4j_host: neo4j
    timeout: 30
  knowledge_graph:
    enabled: false
    neo4j_host: neo4j
    timeout: 30

speaker_recognition:
  # Enable/disable speaker recognition (overrides DISABLE_SPEAKER_RECOGNITION env var)
  enabled: true
  # Service URL (defaults to SPEAKER_SERVICE_URL env var if not specified)
  service_url: null
  # Request timeout in seconds
  timeout: 60
  # Diarization chunking configuration (speaker service self-managed chunking)
  max_diarize_duration: 60  # Maximum audio duration (seconds) for single PyAnnote call
  diarize_chunk_overlap: 5.0  # Overlap (seconds) between chunks
  backend_api_url: http://host.docker.internal:8000  # Backend API URL
