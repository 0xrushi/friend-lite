# Optional Services
# Services that can be enabled via profiles or are commented out by default
# All services below are commented out by default - uncomment as needed

services: {}

# Commented out services below - uncomment as needed:

  # Ollama - Local LLM service
  # Uncomment to use local LLM instead of OpenAI
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   networks:
  #     - chronicle-network
  #   # Uncomment for GPU support:
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - driver: nvidia
  #   #           count: all
  #   #           capabilities: [gpu]

  # Neo4j - Graph database for advanced memory relationships
  # Uncomment if using neo4j-based memory provider
  # neo4j-mem0:
  #   image: neo4j:5.15-community
  #   ports:
  #     - "7474:7474" # HTTP
  #     - "7687:7687" # Bolt
  #   environment:
  #     - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD:-password}
  #     - NEO4J_PLUGINS=["apoc"]
  #     - NEO4J_dbms_security_procedures_unrestricted=apoc.*
  #     - NEO4J_dbms_security_procedures_allowlist=apoc.*
  #   volumes:
  #     - neo4j_data:/data
  #     - neo4j_logs:/logs
  #   restart: unless-stopped
  #   networks:
  #     - chronicle-network

  # OpenMemory MCP Server
  # Uncomment if using openmemory_mcp memory provider
  # openmemory-mcp:
  #   build:
  #     context: ../../extras/openmemory-mcp/cache/mem0/openmemory/api
  #     dockerfile: Dockerfile
  #   env_file:
  #     - .env
  #   environment:
  #     - OPENAI_API_KEY=${OPENAI_API_KEY}
  #     - OPENMEMORY_USER_ID=${OPENMEMORY_USER_ID:-openmemory}
  #   depends_on:
  #     - qdrant
  #   ports:
  #     - "8765:8765"
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD", "python", "-c", "import requests; exit(0 if requests.get('http://localhost:8765/docs').status_code == 200 else 1)"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 30s
  #   networks:
  #     - chronicle-network

  # Ngrok - Expose to internet for testing
  # UNCOMMENT FOR LOCAL DEMO - EXPOSES to internet
  # Use Tailscale instead for production
  # ngrok:
  #   image: ngrok/ngrok:latest
  #   depends_on: [friend-backend, proxy]
  #   ports:
  #     - "4040:4040" # Ngrok web interface
  #   environment:
  #     - NGROK_AUTHTOKEN=${NGROK_AUTHTOKEN}
  #   command: "http proxy:80 --url=${NGROK_URL} --basic-auth=${NGROK_BASIC_AUTH}"
  #   networks:
  #     - chronicle-network

networks:
  chronicle-network:
    name: chronicle-network
    external: true

volumes:
  ollama_data:
    driver: local
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
